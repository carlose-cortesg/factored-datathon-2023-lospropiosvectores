{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a122381b-7163-4a63-96b3-df59286ba955",
   "metadata": {},
   "source": [
    "# Read Syntetic reviews of fashions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c5423e-f16d-439c-8896-0a4fd50a4309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reviews = pd.read_csv('../Syntetic_reviews/reviews_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffc434-3b2f-4a0a-b86b-19a34636a042",
   "metadata": {},
   "source": [
    "# Create V Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8d7ffcbc-d8b5-4a20-bb37-881ac7f68202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found at: /Users/mateograciano/.cache/torch/sentence_transformers/sentence-transformers_all-MiniLM-L6-v2/quantized_true.onnx\n",
      "0.35591753724430286 0.41907971093697205\n",
      "CPU times: user 1min 52s, sys: 891 ms, total: 1min 53s\n",
      "Wall time: 39.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import spacy\n",
    "from fast_sentence_transformers import FastSentenceTransformer as SentenceTransformer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self,nlp,model):\n",
    "        self.vectors = {}\n",
    "        self.nlp = nlp\n",
    "        self.model = model\n",
    "        self.very_similar = 0.5\n",
    "        self.similar = 0.5\n",
    "        \n",
    "\n",
    "    def split_sentences(self, text):\n",
    "        doc = self.nlp(text, disable=[\"ner\"])\n",
    "        roots = [token  for token in doc if token.dep_ == \"ROOT\" ]\n",
    "    \n",
    "        texts = []\n",
    "        for root in roots:\n",
    "            token_list = [e.i for e in root.subtree]\n",
    "            token_list = list(dict.fromkeys(token_list))\n",
    "            token_list.sort()\n",
    "            text = ' '.join([doc[i].text for i in token_list ])\n",
    "            texts.append(text.lower().strip())\n",
    "            \n",
    "        return texts\n",
    "\n",
    "\n",
    "    def insert(self, sentence: str, polarity: int, type: str) -> None:\n",
    "        model = self.model\n",
    "        embeddings = model.encode(sentence)\n",
    "        key = len(self.vectors) + 1\n",
    "        self.vectors[key] = {'text': sentence,\n",
    "                             'polarity': polarity,\n",
    "                             'type': type,\n",
    "                             'vector': embeddings}\n",
    "\n",
    "    def search(self, query: str):\n",
    "        model = self.model\n",
    "        query_vector = model.encode(query)\n",
    "        \n",
    "        similarities = [(key, value['text'],distance.cosine(query_vector, value['vector']),value['polarity'],value['type']) for key, value in self.vectors.items()]\n",
    "        \n",
    "\n",
    "        aux = pd.DataFrame(similarities)\n",
    "        aux.columns = ['index_db','text','similarity','polarity','topic']\n",
    "        aux = aux.sort_values(by=['similarity']).reset_index(drop=True).reset_index()\n",
    "\n",
    "        #aux = aux.reset_index().query('index<20 or similarity<0.7').query('similarity<1')[['index','topic']].groupby(['topic']).count()\n",
    "        \n",
    "        aux = aux.query('index<=10')\n",
    "        #aux = aux.query('similarity <={}'.format(self.very_similar))\n",
    "\n",
    "        aux = aux.query('similarity <={}'.format(self.similar))\n",
    "        \n",
    "        aux = aux[['index','topic']].groupby(['topic']).count()\n",
    "        \n",
    "        \n",
    "        #aux['index2'] = aux['index']/aux['index'].sum()\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        aux = aux.sort_values(by='index', ascending=False).head(1)\n",
    "                \n",
    "        return  list(aux.index.values)\n",
    "\n",
    "    def long_search(self, query: str):\n",
    "        topics = []\n",
    "        for str in self.split_sentences(query):\n",
    "            topics_this = self.search(str)\n",
    "            if len(topics_this)>0:\n",
    "                mini_df = pd.DataFrame(topics_this)\n",
    "                mini_df.columns = ['topic']\n",
    "                mini_df['review'] = query\n",
    "                mini_df['sub_review'] = str\n",
    "                topics.append(mini_df)\n",
    "        if len(topics)>0:\n",
    "            \n",
    "            aux = pd.concat(topics)\n",
    "            #aux ['stars'] = [int(self.sentiment_pipe(str)[0]['label'][0]) for str in aux.sub_review]\n",
    "        else:\n",
    "            aux = None\n",
    "            \n",
    "        return  aux\n",
    "\n",
    "    def set_th(self):\n",
    "        data = pd.DataFrame(self.vectors).transpose()\n",
    "\n",
    "        same_type_similarity = []\n",
    "        \n",
    "        same_type_top_similarity = []\n",
    "        \n",
    "        for i in range(len(data.vector)):\n",
    "        \n",
    "            vectors = data.vector\n",
    "            vector = vectors.values[i]\n",
    "            aux = pd.DataFrame(\n",
    "                [distance.cosine(vector, vectors[i]) for i in vectors.keys()]\n",
    "            )\n",
    "            \n",
    "            aux.columns = ['similarity']\n",
    "            \n",
    "            aux['topic'] = data.type.values\n",
    "            \n",
    "            topic_review = data.type.values[i]\n",
    "             \n",
    "            same_type_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,50))\n",
    "        \n",
    "            same_type_top_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,25))\n",
    "        \n",
    "        self.very_similar = np.mean(same_type_top_similarity)\n",
    "        self.similar = np.mean(same_type_similarity)\n",
    "\n",
    "        print(self.very_similar,self.similar)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# use any sentence-transformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\", quantize=True)\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "    train_reviews = reviews.sample(n=700)\n",
    "    val_reviews = reviews[~reviews.index.isin(train_reviews.index)]\n",
    "    \n",
    "    vector_db = VectorDatabase(nlp, model)\n",
    "    \n",
    "    for index, row in train_reviews.iterrows():\n",
    "        vector_db.insert(row['Review'],row['Polarity'],row['Topic'])\n",
    "    \n",
    "    \n",
    "    vector_db.set_th()\n",
    "    \n",
    "    \n",
    "    guesses = []\n",
    "    \n",
    "    \n",
    "    for index, row in val_reviews.iterrows():\n",
    "        #print(index)\n",
    "        review = row['Review']\n",
    "        aux = vector_db.long_search(review)\n",
    "        guess = []\n",
    "        if aux is not None:\n",
    "            guess = aux.topic.values\n",
    "        guesses.append(guess)\n",
    "        \n",
    "    val_reviews['guesses'] = guesses\n",
    "    \n",
    "    \n",
    "    recalls = []\n",
    "    precisions= []\n",
    "    for index, row in val_reviews.iterrows():\n",
    "        recall = row['Topic'] in row['guesses']\n",
    "        precision = np.nan\n",
    "        if len(row['guesses'])>0:\n",
    "            precision = recall\n",
    "            \n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "    \n",
    "    precision = np.nanmean(np.array(precisions))\n",
    "    recall = np.nanmean(np.array(recalls))\n",
    "    \n",
    "    print('recall: {} precision: {}'.format(recall,precision))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "9122c103-1ce5-408c-8e33-af0a87d98994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.9035087719298246 precision: 0.911504424778761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_v/11c2y9b932l_09kgbz74rhwm0000gn/T/ipykernel_75150/2905657703.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_reviews['guesses'] = guesses\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
