{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a122381b-7163-4a63-96b3-df59286ba955",
   "metadata": {},
   "source": [
    "# Read Syntetic reviews of fashions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c5423e-f16d-439c-8896-0a4fd50a4309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reviews = pd.read_csv('../Syntetic_reviews/reviews_all_aumented_iter1.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "110e7c4f-3d7d-4191-97af-0e67d661772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews = reviews.sample(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffc434-3b2f-4a0a-b86b-19a34636a042",
   "metadata": {},
   "source": [
    "# High Precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d7ffcbc-d8b5-4a20-bb37-881ac7f68202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/factored/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found at: /Users/mateograciano/.cache/torch/sentence_transformers/sentence-transformers_all-MiniLM-L6-v2/quantized_true.onnx\n",
      "Cross validation #1 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "                        topic    recall  precision\n",
      "0                       Brand  0.990654   0.990654\n",
      "1            Customer Support  0.731959   1.000000\n",
      "2                      Design  0.614865   0.866667\n",
      "3             Fit and Comfort  0.507692   0.846154\n",
      "4                   Longevity  0.709677   0.956522\n",
      "5        Material and Quality  0.440678   0.787879\n",
      "6  Packaging and Presentation  0.687500   0.942857\n",
      "7             Price and Value  0.795918   0.975000\n",
      "8             User Experience  0.520000   0.981132\n",
      "9                 Versatility  0.539683   0.971429\n",
      "0.9318293035194625 0.6538626433537258\n",
      "Cross validation #2 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "                        topic    recall  precision\n",
      "0                       Brand  0.979592   1.000000\n",
      "1            Customer Support  0.774194   0.960000\n",
      "2                      Design  0.673759   0.805085\n",
      "3             Fit and Comfort  0.561404   0.888889\n",
      "4                   Longevity  0.790698   0.944444\n",
      "5        Material and Quality  0.387500   0.939394\n",
      "6  Packaging and Presentation  0.592593   0.941176\n",
      "7             Price and Value  0.813725   0.943182\n",
      "8             User Experience  0.456311   1.000000\n",
      "9                 Versatility  0.542857   0.950000\n",
      "0.9372170307260037 0.6572631338818017\n",
      "Cross validation #3 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "                        topic    recall  precision\n",
      "0                       Brand  0.990741   1.000000\n",
      "1            Customer Support  0.728972   0.917647\n",
      "2                      Design  0.679104   0.883495\n",
      "3             Fit and Comfort  0.472973   0.945946\n",
      "4                   Longevity  0.770833   0.973684\n",
      "5        Material and Quality  0.289855   0.909091\n",
      "6  Packaging and Presentation  0.726496   0.965909\n",
      "7             Price and Value  0.789474   0.974026\n",
      "8             User Experience  0.421687   0.875000\n",
      "9                 Versatility  0.516667   0.968750\n",
      "0.9413548334952833 0.6386801384100449\n",
      "Cross validation #4 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "                        topic    recall  precision\n",
      "0                       Brand  0.990826   0.990826\n",
      "1            Customer Support  0.741573   0.956522\n",
      "2                      Design  0.682759   0.838983\n",
      "3             Fit and Comfort  0.469697   0.815789\n",
      "4                   Longevity  0.745098   1.000000\n",
      "5        Material and Quality  0.437500   0.903226\n",
      "6  Packaging and Presentation  0.673077   0.945946\n",
      "7             Price and Value  0.792079   0.963855\n",
      "8             User Experience  0.403846   0.954545\n",
      "9                 Versatility  0.516129   0.941176\n",
      "0.9310869050953494 0.6452583668485505\n",
      "Cross validation #5 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "                        topic    recall  precision\n",
      "0                       Brand  0.990826   1.000000\n",
      "1            Customer Support  0.697674   0.967742\n",
      "2                      Design  0.722222   0.873950\n",
      "3             Fit and Comfort  0.442857   0.939394\n",
      "4                   Longevity  0.792453   0.954545\n",
      "5        Material and Quality  0.350649   0.931034\n",
      "6  Packaging and Presentation  0.663462   0.932432\n",
      "7             Price and Value  0.815534   0.943820\n",
      "8             User Experience  0.468354   0.880952\n",
      "9                 Versatility  0.585714   0.931818\n",
      "0.9355688611935914 0.6529745887733536\n",
      "CPU times: user 2h 1min 41s, sys: 28.3 s, total: 2h 2min 9s\n",
      "Wall time: 59min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import spacy\n",
    "from fast_sentence_transformers import FastSentenceTransformer as SentenceTransformer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self,nlp,model):\n",
    "        self.vectors = {}\n",
    "        self.nlp = nlp\n",
    "        self.model = model\n",
    "        self.very_similar = 0.5\n",
    "        self.similar = 0.5\n",
    "        \n",
    "\n",
    "    def split_sentences(self, text):\n",
    "        doc = self.nlp(text, disable=[\"ner\"])\n",
    "        roots = [token  for token in doc if token.dep_ == \"ROOT\" ]\n",
    "    \n",
    "        texts = []\n",
    "        for root in roots:\n",
    "            token_list = [e.i for e in root.subtree]\n",
    "            token_list = list(dict.fromkeys(token_list))\n",
    "            token_list.sort()\n",
    "            text = ' '.join([doc[i].text for i in token_list ])\n",
    "            texts.append(text.lower().strip())\n",
    "            \n",
    "        return texts\n",
    "\n",
    "\n",
    "    def insert(self, sentence: str, polarity: int, type: str) -> None:\n",
    "        model = self.model\n",
    "        embeddings = model.encode(sentence)\n",
    "        key = len(self.vectors) + 1\n",
    "        self.vectors[key] = {'text': sentence,\n",
    "                             'polarity': polarity,\n",
    "                             'type': type,\n",
    "                             'vector': embeddings}\n",
    "\n",
    "    def search(self, query: str):\n",
    "        model = self.model\n",
    "        query_vector = model.encode(query)\n",
    "        \n",
    "        similarities = [(key, value['text'],distance.cosine(query_vector, value['vector']),value['polarity'],value['type']) for key, value in self.vectors.items()]\n",
    "        \n",
    "\n",
    "        aux = pd.DataFrame(similarities)\n",
    "        aux.columns = ['index_db','text','similarity','polarity','topic']\n",
    "        aux = aux.sort_values(by=['similarity']).reset_index(drop=True).reset_index()\n",
    "\n",
    "        #aux = aux.reset_index().query('index<20 or similarity<0.7').query('similarity<1')[['index','topic']].groupby(['topic']).count()\n",
    "        \n",
    "        aux = aux.query('index<=10')\n",
    "        #aux = aux.query('similarity <={}'.format(self.very_similar))\n",
    "\n",
    "        aux = aux.query('similarity <={}'.format(self.similar))\n",
    "        \n",
    "        aux = aux[['index','topic']].groupby(['topic']).count()\n",
    "        \n",
    "        \n",
    "        aux['index2'] = aux['index']/aux['index'].sum()\n",
    "     \n",
    "        aux = aux.query('index2>0.6')\n",
    "        \n",
    "\n",
    "        aux = aux.sort_values(by='index', ascending=False).head(1)\n",
    "                \n",
    "        return  list(aux.index.values)\n",
    "\n",
    "    def long_search(self, query: str):\n",
    "        topics = []\n",
    "        for str in self.split_sentences(query):\n",
    "            topics_this = self.search(str)\n",
    "            if len(topics_this)>0:\n",
    "                mini_df = pd.DataFrame(topics_this)\n",
    "                mini_df.columns = ['topic']\n",
    "                mini_df['review'] = query\n",
    "                mini_df['sub_review'] = str\n",
    "                topics.append(mini_df)\n",
    "        if len(topics)>0:\n",
    "            \n",
    "            aux = pd.concat(topics)\n",
    "            #aux ['stars'] = [int(self.sentiment_pipe(str)[0]['label'][0]) for str in aux.sub_review]\n",
    "        else:\n",
    "            aux = None\n",
    "            \n",
    "        return  aux\n",
    "\n",
    "    def set_th(self):\n",
    "        data = pd.DataFrame(self.vectors).transpose()\n",
    "\n",
    "\n",
    "        n = data.shape[0]\n",
    "        if(n>1000):\n",
    "            n = min(max(int(n*0.1),1000),n)\n",
    "            data = data.sample(n)\n",
    "        print('Total size to compute the is {}'.format(data.shape[0]))\n",
    "\n",
    "        same_type_similarity = []\n",
    "        \n",
    "        same_type_top_similarity = []\n",
    "        \n",
    "        for i in range(len(data.vector)):\n",
    "            \n",
    "        \n",
    "            vectors = data.vector\n",
    "            vector = vectors.values[i]\n",
    "            aux = pd.DataFrame(\n",
    "                [distance.cosine(vector, vectors[i]) for i in vectors.keys()]\n",
    "            )\n",
    "            \n",
    "            aux.columns = ['similarity']\n",
    "            \n",
    "            aux['topic'] = data.type.values\n",
    "            \n",
    "            topic_review = data.type.values[i]\n",
    "             \n",
    "            same_type_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,75))\n",
    "        \n",
    "            same_type_top_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,25))\n",
    "        \n",
    "        self.very_similar = np.percentile(same_type_top_similarity,95)\n",
    "        self.similar = np.percentile(same_type_similarity,95)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# use any sentence-transformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\", quantize=True)\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "for db in range(5):\n",
    "\n",
    "    print(f'Cross validation #{db+1} of 5')\n",
    "    sample_size = int(reviews.shape[0]*0.9)\n",
    "    train_reviews = reviews.sample(n=sample_size)\n",
    "    val_reviews = reviews[~reviews.index.isin(train_reviews.index)]\n",
    "    \n",
    "    vector_db = VectorDatabase(nlp, model)\n",
    "    print('uploading vectors to DB')\n",
    "    for index, row in train_reviews.iterrows():\n",
    "        vector_db.insert(row['Review'],None,row['Topic'])\n",
    "    \n",
    "    print('setting thresholds')\n",
    "    vector_db.set_th()\n",
    "    print('seted')\n",
    "    \n",
    "    \n",
    "    guess = [] \n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    for index, row in val_reviews.iterrows():\n",
    "        aux = vector_db.long_search(row['Review'])\n",
    "        my_guess = []\n",
    "        if aux is not None:\n",
    "            my_guess = list(aux.topic.unique())\n",
    "        real = [row.Topic]\n",
    "        if len(real)>0:\n",
    "            #real = list(row.dropna().index)[1:] \n",
    "            recall = pd.DataFrame(real)\n",
    "            recall.columns = ['topic']\n",
    "            recall['value'] = [ t in my_guess for t in real]\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        if len(my_guess)>0:\n",
    "            precision = pd.DataFrame(my_guess)\n",
    "            precision.columns = ['topic']\n",
    "            precision['value'] = [ t in real for t in my_guess]\n",
    "            precisions.append(precision)\n",
    "    \n",
    "    recall = pd.concat(recalls).groupby(['topic']).mean().reset_index()\n",
    "    precision = pd.concat(precisions).groupby(['topic']).mean().reset_index()\n",
    "    \n",
    "    metrics = recall.merge(precision, on = ['topic'])\n",
    "    \n",
    "    metrics.columns = ['topic','recall','precision']\n",
    "    \n",
    "    print(metrics)\n",
    "    print(precision.value.mean(),recall.value.mean())\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff311ca2-152a-43d3-906b-aeedc7253de5",
   "metadata": {},
   "source": [
    "# High Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9122c103-1ce5-408c-8e33-af0a87d98994",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (<unknown>, line 128)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/opt/anaconda3/envs/factored/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3508\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[3], line 1\u001b[0m\n    get_ipython().run_cell_magic('time', '', 'import numpy as np\\nfrom scipy.spatial import distance\\nfrom collections import defaultdict\\nfrom typing import List, Tuple\\nimport spacy\\nfrom fast_sentence_transformers import FastSentenceTransformer as SentenceTransformer\\n\\n\\nimport numpy as np\\n\\n\\nclass VectorDatabase:\\n    def __init__(self,nlp,model):\\n        self.vectors = {}\\n        self.nlp = nlp\\n        self.model = model\\n        self.very_similar = 0.5\\n        self.similar = 0.5\\n        \\n\\n    def split_sentences(self, text):\\n        doc = self.nlp(text, disable=[\"ner\"])\\n        roots = [token  for token in doc if token.dep_ == \"ROOT\" ]\\n    \\n        texts = []\\n        for root in roots:\\n            token_list = [e.i for e in root.subtree]\\n            token_list = list(dict.fromkeys(token_list))\\n            token_list.sort()\\n            text = \\' \\'.join([doc[i].text for i in token_list ])\\n            texts.append(text.lower().strip())\\n            \\n        return texts\\n\\n\\n    def insert(self, sentence: str, polarity: int, type: str) -> None:\\n        model = self.model\\n        embeddings = model.encode(sentence)\\n        key = len(self.vectors) + 1\\n        self.vectors[key] = {\\'text\\': sentence,\\n                             \\'polarity\\': polarity,\\n                             \\'type\\': type,\\n                             \\'vector\\': embeddings}\\n\\n    def search(self, query: str):\\n        model = self.model\\n        query_vector = model.encode(query)\\n        \\n        similarities = [(key, value[\\'text\\'],distance.cosine(query_vector, value[\\'vector\\']),value[\\'polarity\\'],value[\\'type\\']) for key, value in self.vectors.items()]\\n        \\n\\n        aux = pd.DataFrame(similarities)\\n        aux.columns = [\\'index_db\\',\\'text\\',\\'similarity\\',\\'polarity\\',\\'topic\\']\\n        aux = aux.sort_values(by=[\\'similarity\\']).reset_index(drop=True).reset_index()\\n\\n        #aux = aux.reset_index().query(\\'index<20 or similarity<0.7\\').query(\\'similarity<1\\')[[\\'index\\',\\'topic\\']].groupby([\\'topic\\']).count()\\n        \\n        aux = aux.query(\\'index<=10\\')\\n        #aux = aux.query(\\'similarity <={}\\'.format(self.very_similar))\\n\\n        aux = aux.query(\\'similarity <={}\\'.format(self.similar))\\n        \\n        aux = aux[[\\'index\\',\\'topic\\']].groupby([\\'topic\\']).count()\\n        \\n        \\n        aux[\\'index2\\'] = aux[\\'index\\']/aux[\\'index\\'].sum()\\n     \\n        aux = aux.query(\\'index2>0.6\\')\\n        \\n\\n        aux = aux.sort_values(by=\\'index\\', ascending=False).head(1)\\n                \\n        return  list(aux.index.values)\\n\\n    def long_search(self, query: str):\\n        topics = []\\n        for str in self.split_sentences(query):\\n            topics_this = self.search(str)\\n            if len(topics_this)>0:\\n                mini_df = pd.DataFrame(topics_this)\\n                mini_df.columns = [\\'topic\\']\\n                mini_df[\\'review\\'] = query\\n                mini_df[\\'sub_review\\'] = str\\n                topics.append(mini_df)\\n        if len(topics)>0:\\n            \\n            aux = pd.concat(topics)\\n            #aux [\\'stars\\'] = [int(self.sentiment_pipe(str)[0][\\'label\\'][0]) for str in aux.sub_review]\\n        else:\\n            aux = None\\n            \\n        return  aux\\n\\n    def set_th(self):\\n        data = pd.DataFrame(self.vectors).transpose()\\n\\n\\n        n = data.shape[0]\\n        if(n>1000):\\n            n = min(max(int(n*0.1),1000),n)\\n            data = data.sample(n)\\n        print(\\'Total size to compute the is {}\\'.format(data.shape[0]))\\n\\n        same_type_similarity = []\\n        \\n        same_type_top_similarity = []\\n        \\n        for i in range(len(data.vector)):\\n            \\n        \\n            vectors = data.vector\\n            vector = vectors.values[i]\\n            aux = pd.DataFrame(\\n                [distance.cosine(vector, vectors[i]) for i in vectors.keys()]\\n            )\\n            \\n            aux.columns = [\\'similarity\\']\\n            \\n            aux[\\'topic\\'] = data.type.values\\n            \\n            topic_review = data.type.values[i]\\n             \\n            same_type_similarity.append(np.percentile(aux.query(f\\'topic==\"{topic_review}\"\\').similarity,75))\\n        \\n            same_type_top_similarity.append(np.percentile(aux.query(f\\'topic==\"{topic_review}\"\\').similarity,25))\\n        \\n        self.very_similar = np.percentile(same_type_top_similarity,50)\\n        self.similar = np.percentile((same_type_similarity,50)\\n\\n\\n\\n\\n# use any sentence-transformer\\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\", quantize=True)\\n\\n\\nnlp = spacy.load(\"en_core_web_lg\")\\n\\n\\n\\nresults = []\\nfor db in range(5):\\n\\n    print(f\\'Cross validation #{db+1} of 5\\')\\n    sample_size = int(reviews.shape[0]*0.9)\\n    train_reviews = reviews.sample(n=sample_size)\\n    val_reviews = reviews[~reviews.index.isin(train_reviews.index)]\\n    \\n    vector_db = VectorDatabase(nlp, model)\\n    print(\\'uploading vectors to DB\\')\\n    for index, row in train_reviews.iterrows():\\n        vector_db.insert(row[\\'Review\\'],None,row[\\'Topic\\'])\\n    \\n    print(\\'setting thresholds\\')\\n    vector_db.set_th()\\n    print(\\'seted\\')\\n    \\n    \\n    guess = [] \\n    recalls = []\\n    precisions = []\\n    for index, row in val_reviews.iterrows():\\n        aux = vector_db.long_search(row[\\'Review\\'])\\n        my_guess = []\\n        if aux is not None:\\n            my_guess = list(aux.topic.unique())\\n        real = [row.Topic]\\n        if len(real)>0:\\n            #real = list(row.dropna().index)[1:] \\n            recall = pd.DataFrame(real)\\n            recall.columns = [\\'topic\\']\\n            recall[\\'value\\'] = [ t in my_guess for t in real]\\n            recalls.append(recall)\\n        \\n        if len(my_guess)>0:\\n            precision = pd.DataFrame(my_guess)\\n            precision.columns = [\\'topic\\']\\n            precision[\\'value\\'] = [ t in real for t in my_guess]\\n            precisions.append(precision)\\n    \\n    recall = pd.concat(recalls).groupby([\\'topic\\']).mean().reset_index()\\n    precision = pd.concat(precisions).groupby([\\'topic\\']).mean().reset_index()\\n    \\n    metrics = recall.merge(precision, on = [\\'topic\\'])\\n    \\n    metrics.columns = [\\'topic\\',\\'recall\\',\\'precision\\']\\n    \\n    print(metrics)\\n    print(precision.value.mean(),recall.value.mean())\\n\\npd.DataFrame(results, columns = [\\'recall\\',\\'precision\\'])\\n')\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/opt/anaconda3/envs/factored/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2478\u001b[0m in \u001b[1;35mrun_cell_magic\u001b[0m\n    result = fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/opt/anaconda3/envs/factored/lib/python3.11/site-packages/IPython/core/magics/execution.py:1281\u001b[0m in \u001b[1;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m/opt/anaconda3/envs/factored/lib/python3.11/site-packages/IPython/core/compilerop.py:86\u001b[0;36m in \u001b[0;35mast_parse\u001b[0;36m\n\u001b[0;31m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<unknown>:128\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.similar = np.percentile((same_type_similarity,50)\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import spacy\n",
    "from fast_sentence_transformers import FastSentenceTransformer as SentenceTransformer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self,nlp,model):\n",
    "        self.vectors = {}\n",
    "        self.nlp = nlp\n",
    "        self.model = model\n",
    "        self.very_similar = 0.5\n",
    "        self.similar = 0.5\n",
    "        \n",
    "\n",
    "    def split_sentences(self, text):\n",
    "        doc = self.nlp(text, disable=[\"ner\"])\n",
    "        roots = [token  for token in doc if token.dep_ == \"ROOT\" ]\n",
    "    \n",
    "        texts = []\n",
    "        for root in roots:\n",
    "            token_list = [e.i for e in root.subtree]\n",
    "            token_list = list(dict.fromkeys(token_list))\n",
    "            token_list.sort()\n",
    "            text = ' '.join([doc[i].text for i in token_list ])\n",
    "            texts.append(text.lower().strip())\n",
    "            \n",
    "        return texts\n",
    "\n",
    "\n",
    "    def insert(self, sentence: str, polarity: int, type: str) -> None:\n",
    "        model = self.model\n",
    "        embeddings = model.encode(sentence)\n",
    "        key = len(self.vectors) + 1\n",
    "        self.vectors[key] = {'text': sentence,\n",
    "                             'polarity': polarity,\n",
    "                             'type': type,\n",
    "                             'vector': embeddings}\n",
    "\n",
    "    def search(self, query: str):\n",
    "        model = self.model\n",
    "        query_vector = model.encode(query)\n",
    "        \n",
    "        similarities = [(key, value['text'],distance.cosine(query_vector, value['vector']),value['polarity'],value['type']) for key, value in self.vectors.items()]\n",
    "        \n",
    "\n",
    "        aux = pd.DataFrame(similarities)\n",
    "        aux.columns = ['index_db','text','similarity','polarity','topic']\n",
    "        aux = aux.sort_values(by=['similarity']).reset_index(drop=True).reset_index()\n",
    "\n",
    "        #aux = aux.reset_index().query('index<20 or similarity<0.7').query('similarity<1')[['index','topic']].groupby(['topic']).count()\n",
    "        \n",
    "        aux = aux.query('index<=10')\n",
    "        #aux = aux.query('similarity <={}'.format(self.very_similar))\n",
    "\n",
    "        aux = aux.query('similarity <={}'.format(self.similar))\n",
    "        \n",
    "        aux = aux[['index','topic']].groupby(['topic']).count()\n",
    "        \n",
    "        \n",
    "        aux['index2'] = aux['index']/aux['index'].sum()\n",
    "     \n",
    "        aux = aux.query('index2>0.6')\n",
    "        \n",
    "\n",
    "        aux = aux.sort_values(by='index', ascending=False).head(1)\n",
    "                \n",
    "        return  list(aux.index.values)\n",
    "\n",
    "    def long_search(self, query: str):\n",
    "        topics = []\n",
    "        for str in self.split_sentences(query):\n",
    "            topics_this = self.search(str)\n",
    "            if len(topics_this)>0:\n",
    "                mini_df = pd.DataFrame(topics_this)\n",
    "                mini_df.columns = ['topic']\n",
    "                mini_df['review'] = query\n",
    "                mini_df['sub_review'] = str\n",
    "                topics.append(mini_df)\n",
    "        if len(topics)>0:\n",
    "            \n",
    "            aux = pd.concat(topics)\n",
    "            #aux ['stars'] = [int(self.sentiment_pipe(str)[0]['label'][0]) for str in aux.sub_review]\n",
    "        else:\n",
    "            aux = None\n",
    "            \n",
    "        return  aux\n",
    "\n",
    "    def set_th(self):\n",
    "        data = pd.DataFrame(self.vectors).transpose()\n",
    "\n",
    "\n",
    "        n = data.shape[0]\n",
    "        if(n>1000):\n",
    "            n = min(max(int(n*0.1),1000),n)\n",
    "            data = data.sample(n)\n",
    "        print('Total size to compute the is {}'.format(data.shape[0]))\n",
    "\n",
    "        same_type_similarity = []\n",
    "        \n",
    "        same_type_top_similarity = []\n",
    "        \n",
    "        for i in range(len(data.vector)):\n",
    "            \n",
    "        \n",
    "            vectors = data.vector\n",
    "            vector = vectors.values[i]\n",
    "            aux = pd.DataFrame(\n",
    "                [distance.cosine(vector, vectors[i]) for i in vectors.keys()]\n",
    "            )\n",
    "            \n",
    "            aux.columns = ['similarity']\n",
    "            \n",
    "            aux['topic'] = data.type.values\n",
    "            \n",
    "            topic_review = data.type.values[i]\n",
    "             \n",
    "            same_type_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,75))\n",
    "        \n",
    "            same_type_top_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,25))\n",
    "        \n",
    "        self.very_similar = np.percentile(same_type_top_similarity,50)\n",
    "        self.similar = np.percentile((same_type_similarity,50)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# use any sentence-transformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\", quantize=True)\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "for db in range(5):\n",
    "\n",
    "    print(f'Cross validation #{db+1} of 5')\n",
    "    sample_size = int(reviews.shape[0]*0.9)\n",
    "    train_reviews = reviews.sample(n=sample_size)\n",
    "    val_reviews = reviews[~reviews.index.isin(train_reviews.index)]\n",
    "    \n",
    "    vector_db = VectorDatabase(nlp, model)\n",
    "    print('uploading vectors to DB')\n",
    "    for index, row in train_reviews.iterrows():\n",
    "        vector_db.insert(row['Review'],None,row['Topic'])\n",
    "    \n",
    "    print('setting thresholds')\n",
    "    vector_db.set_th()\n",
    "    print('seted')\n",
    "    \n",
    "    \n",
    "    guess = [] \n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    for index, row in val_reviews.iterrows():\n",
    "        aux = vector_db.long_search(row['Review'])\n",
    "        my_guess = []\n",
    "        if aux is not None:\n",
    "            my_guess = list(aux.topic.unique())\n",
    "        real = [row.Topic]\n",
    "        if len(real)>0:\n",
    "            #real = list(row.dropna().index)[1:] \n",
    "            recall = pd.DataFrame(real)\n",
    "            recall.columns = ['topic']\n",
    "            recall['value'] = [ t in my_guess for t in real]\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        if len(my_guess)>0:\n",
    "            precision = pd.DataFrame(my_guess)\n",
    "            precision.columns = ['topic']\n",
    "            precision['value'] = [ t in real for t in my_guess]\n",
    "            precisions.append(precision)\n",
    "    \n",
    "    recall = pd.concat(recalls).groupby(['topic']).mean().reset_index()\n",
    "    precision = pd.concat(precisions).groupby(['topic']).mean().reset_index()\n",
    "    \n",
    "    metrics = recall.merge(precision, on = ['topic'])\n",
    "    \n",
    "    metrics.columns = ['topic','recall','precision']\n",
    "    \n",
    "    print(metrics)\n",
    "    print(precision.value.mean(),recall.value.mean())\n",
    "\n",
    "pd.DataFrame(results, columns = ['recall','precision'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c93e49-df54-42ff-8b91-c9e6e4b2197a",
   "metadata": {},
   "source": [
    "# High recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71e2e850-0c23-4b16-937d-14f5a7226d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found at: /Users/mateograciano/.cache/torch/sentence_transformers/sentence-transformers_all-MiniLM-L6-v2/quantized_true.onnx\n",
      "Cross validation #1 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "                        topic    recall  precision\n",
      "0                       Brand  1.000000   0.971154\n",
      "1            Customer Support  0.797753   0.816092\n",
      "2                      Design  0.912752   0.576271\n",
      "3             Fit and Comfort  0.761905   0.813559\n",
      "4                   Longevity  0.857143   0.960000\n",
      "5        Material and Quality  0.696970   0.707692\n",
      "6  Packaging and Presentation  0.702970   0.771739\n",
      "7             Price and Value  0.923913   0.850000\n",
      "8             User Experience  0.645455   0.739583\n",
      "9                 Versatility  0.720588   0.890909\n",
      "0.8097000171020925 0.8019447924115056\n",
      "Cross validation #2 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "                        topic    recall  precision\n",
      "0                       Brand  0.990654   0.981481\n",
      "1            Customer Support  0.884956   0.877193\n",
      "2                      Design  0.888112   0.628713\n",
      "3             Fit and Comfort  0.724138   0.711864\n",
      "4                   Longevity  0.921053   0.875000\n",
      "5        Material and Quality  0.617647   0.736842\n",
      "6  Packaging and Presentation  0.839623   0.773913\n",
      "7             Price and Value  0.937500   0.849057\n",
      "8             User Experience  0.593407   0.857143\n",
      "9                 Versatility  0.880000   0.776471\n",
      "0.8067676939897567 0.8277088702284742\n",
      "Cross validation #3 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "                        topic    recall  precision\n",
      "0                       Brand  1.000000   0.991304\n",
      "1            Customer Support  0.861702   0.826531\n",
      "2                      Design  0.899281   0.584112\n",
      "3             Fit and Comfort  0.718310   0.728571\n",
      "4                   Longevity  0.914894   0.895833\n",
      "5        Material and Quality  0.671429   0.770492\n",
      "6  Packaging and Presentation  0.780488   0.800000\n",
      "7             Price and Value  0.926316   0.888889\n",
      "8             User Experience  0.578313   0.676056\n",
      "9                 Versatility  0.745763   0.814815\n",
      "0.7976603716519018 0.8096494310032109\n",
      "Cross validation #4 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "                        topic    recall  precision\n",
      "0                       Brand  0.990000   0.961165\n",
      "1            Customer Support  0.818182   0.778846\n",
      "2                      Design  0.890244   0.626609\n",
      "3             Fit and Comfort  0.666667   0.666667\n",
      "4                   Longevity  0.907407   0.907407\n",
      "5        Material and Quality  0.633333   0.690909\n",
      "6  Packaging and Presentation  0.792453   0.777778\n",
      "7             Price and Value  0.891304   0.881720\n",
      "8             User Experience  0.636364   0.677419\n",
      "9                 Versatility  0.688525   0.792453\n",
      "0.7760974202345788 0.7914478532570588\n",
      "Cross validation #5 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "                        topic    recall  precision\n",
      "0                       Brand  0.981818   0.981818\n",
      "1            Customer Support  0.883495   0.875000\n",
      "2                      Design  0.899329   0.598214\n",
      "3             Fit and Comfort  0.759259   0.640625\n",
      "4                   Longevity  0.924528   0.942308\n",
      "5        Material and Quality  0.636364   0.724138\n",
      "6  Packaging and Presentation  0.828571   0.836538\n",
      "7             Price and Value  0.885417   0.841584\n",
      "8             User Experience  0.685393   0.824324\n",
      "9                 Versatility  0.642857   0.803571\n",
      "0.8068121463724699 0.8127031880541544\n",
      "CPU times: user 52min 13s, sys: 8.89 s, total: 52min 21s\n",
      "Wall time: 20min 34s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [recall, precision]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import spacy\n",
    "from fast_sentence_transformers import FastSentenceTransformer as SentenceTransformer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self,nlp,model):\n",
    "        self.vectors = {}\n",
    "        self.nlp = nlp\n",
    "        self.model = model\n",
    "        self.very_similar = 0.5\n",
    "        self.similar = 0.5\n",
    "        \n",
    "\n",
    "    def split_sentences(self, text):\n",
    "        doc = self.nlp(text, disable=[\"ner\"])\n",
    "        roots = [token  for token in doc if token.dep_ == \"ROOT\" ]\n",
    "    \n",
    "        texts = []\n",
    "        for root in roots:\n",
    "            token_list = [e.i for e in root.subtree]\n",
    "            token_list = list(dict.fromkeys(token_list))\n",
    "            token_list.sort()\n",
    "            text = ' '.join([doc[i].text for i in token_list ])\n",
    "            texts.append(text.lower().strip())\n",
    "            \n",
    "        return texts\n",
    "\n",
    "\n",
    "    def insert(self, sentence: str, polarity: int, type: str) -> None:\n",
    "        model = self.model\n",
    "        embeddings = model.encode(sentence)\n",
    "        key = len(self.vectors) + 1\n",
    "        self.vectors[key] = {'text': sentence,\n",
    "                             'polarity': polarity,\n",
    "                             'type': type,\n",
    "                             'vector': embeddings}\n",
    "\n",
    "    def search(self, query: str):\n",
    "        model = self.model\n",
    "        query_vector = model.encode(query)\n",
    "        \n",
    "        similarities = [(key, value['text'],distance.cosine(query_vector, value['vector']),value['polarity'],value['type']) for key, value in self.vectors.items()]\n",
    "        \n",
    "\n",
    "        aux = pd.DataFrame(similarities)\n",
    "        aux.columns = ['index_db','text','similarity','polarity','topic']\n",
    "        aux = aux.sort_values(by=['similarity']).reset_index(drop=True).reset_index()\n",
    "\n",
    "        #aux = aux.reset_index().query('index<20 or similarity<0.7').query('similarity<1')[['index','topic']].groupby(['topic']).count()\n",
    "        \n",
    "        aux = aux.query('index<=10')\n",
    "        #aux = aux.query('similarity <={}'.format(self.very_similar))\n",
    "\n",
    "        aux = aux.query('similarity <={}'.format(self.similar))\n",
    "        \n",
    "        aux = aux[['index','topic']].groupby(['topic']).count()\n",
    "        \n",
    "        \n",
    "        aux['index2'] = aux['index']/aux['index'].sum()\n",
    "     \n",
    "        #aux = aux.query('index2>0.6')\n",
    "        \n",
    "\n",
    "        aux = aux.sort_values(by='index', ascending=False).head(1)\n",
    "                \n",
    "        return  list(aux.index.values)\n",
    "\n",
    "    def long_search(self, query: str):\n",
    "        topics = []\n",
    "        for str in self.split_sentences(query):\n",
    "            topics_this = self.search(str)\n",
    "            if len(topics_this)>0:\n",
    "                mini_df = pd.DataFrame(topics_this)\n",
    "                mini_df.columns = ['topic']\n",
    "                mini_df['review'] = query\n",
    "                mini_df['sub_review'] = str\n",
    "                topics.append(mini_df)\n",
    "        if len(topics)>0:\n",
    "            \n",
    "            aux = pd.concat(topics)\n",
    "            #aux ['stars'] = [int(self.sentiment_pipe(str)[0]['label'][0]) for str in aux.sub_review]\n",
    "        else:\n",
    "            aux = None\n",
    "            \n",
    "        return  aux\n",
    "\n",
    "    def set_th(self):\n",
    "        data = pd.DataFrame(self.vectors).transpose()\n",
    "\n",
    "\n",
    "        n = data.shape[0]\n",
    "        if(n>1000):\n",
    "            n = min(max(int(n*0.1),1000),n)\n",
    "            data = data.sample(n)\n",
    "        print('Total size to compute the is {}'.format(data.shape[0]))\n",
    "\n",
    "        same_type_similarity = []\n",
    "        \n",
    "        same_type_top_similarity = []\n",
    "        \n",
    "        for i in range(len(data.vector)):\n",
    "            \n",
    "        \n",
    "            vectors = data.vector\n",
    "            vector = vectors.values[i]\n",
    "            aux = pd.DataFrame(\n",
    "                [distance.cosine(vector, vectors[i]) for i in vectors.keys()]\n",
    "            )\n",
    "            \n",
    "            aux.columns = ['similarity']\n",
    "            \n",
    "            aux['topic'] = data.type.values\n",
    "            \n",
    "            topic_review = data.type.values[i]\n",
    "             \n",
    "            same_type_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,75))\n",
    "        \n",
    "            same_type_top_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,25))\n",
    "        \n",
    "        self.very_similar = np.percentile(same_type_top_similarity,75)\n",
    "        self.similar = np.mean(same_type_similarity)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# use any sentence-transformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\", quantize=True)\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "for db in range(5):\n",
    "\n",
    "    print(f'Cross validation #{db+1} of 5')\n",
    "    sample_size = int(reviews.shape[0]*0.9)\n",
    "    train_reviews = reviews.sample(n=sample_size)\n",
    "    val_reviews = reviews[~reviews.index.isin(train_reviews.index)]\n",
    "    \n",
    "    vector_db = VectorDatabase(nlp, model)\n",
    "    print('uploading vectors to DB')\n",
    "    for index, row in train_reviews.iterrows():\n",
    "        vector_db.insert(row['Review'],None,row['Topic'])\n",
    "    \n",
    "    print('setting thresholds')\n",
    "    vector_db.set_th()\n",
    "    print('seted')\n",
    "    \n",
    "    \n",
    "    guess = [] \n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    for index, row in val_reviews.iterrows():\n",
    "        aux = vector_db.long_search(row['Review'])\n",
    "        my_guess = []\n",
    "        if aux is not None:\n",
    "            my_guess = list(aux.topic.unique())\n",
    "        real = [row.Topic]\n",
    "        if len(real)>0:\n",
    "            #real = list(row.dropna().index)[1:] \n",
    "            recall = pd.DataFrame(real)\n",
    "            recall.columns = ['topic']\n",
    "            recall['value'] = [ t in my_guess for t in real]\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        if len(my_guess)>0:\n",
    "            precision = pd.DataFrame(my_guess)\n",
    "            precision.columns = ['topic']\n",
    "            precision['value'] = [ t in real for t in my_guess]\n",
    "            precisions.append(precision)\n",
    "    \n",
    "    recall = pd.concat(recalls).groupby(['topic']).mean().reset_index()\n",
    "    precision = pd.concat(precisions).groupby(['topic']).mean().reset_index()\n",
    "    \n",
    "    metrics = recall.merge(precision, on = ['topic'])\n",
    "    \n",
    "    metrics.columns = ['topic','recall','precision']\n",
    "    \n",
    "    print(metrics)\n",
    "    print(precision.value.mean(),recall.value.mean())\n",
    "\n",
    "pd.DataFrame(results, columns = ['recall','precision'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
