{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a122381b-7163-4a63-96b3-df59286ba955",
   "metadata": {},
   "source": [
    "# Read Syntetic reviews of fashions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c5423e-f16d-439c-8896-0a4fd50a4309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reviews = pd.read_csv('../Syntetic_reviews/reviews_all_aumented_iter1.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "110e7c4f-3d7d-4191-97af-0e67d661772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews = reviews.sample(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffc434-3b2f-4a0a-b86b-19a34636a042",
   "metadata": {},
   "source": [
    "# High Precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d7ffcbc-d8b5-4a20-bb37-881ac7f68202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/factored/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found at: /Users/mateograciano/.cache/torch/sentence_transformers/sentence-transformers_all-MiniLM-L6-v2/quantized_true.onnx\n",
      "Cross validation #1 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "Making Classifications\n",
      "895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.7027932960893855 precision: 0.9515885022692889\n",
      "Cross validation #2 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "Making Classifications\n",
      "895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.6737430167597765 precision: 0.9466248037676609\n",
      "Cross validation #3 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "Making Classifications\n",
      "895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.6804469273743017 precision: 0.9471228615863142\n",
      "Cross validation #4 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "Making Classifications\n",
      "895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.6547486033519553 precision: 0.9406099518459069\n",
      "Cross validation #5 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "Making Classifications\n",
      "895\n",
      "recall: 0.693854748603352 precision: 0.9324324324324325\n",
      "     recall  precision\n",
      "0  0.702793   0.951589\n",
      "1  0.673743   0.946625\n",
      "2  0.680447   0.947123\n",
      "3  0.654749   0.940610\n",
      "4  0.693855   0.932432\n",
      "CPU times: user 54min 17s, sys: 12.4 s, total: 54min 30s\n",
      "Wall time: 22min 12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import spacy\n",
    "from fast_sentence_transformers import FastSentenceTransformer as SentenceTransformer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self,nlp,model):\n",
    "        self.vectors = {}\n",
    "        self.nlp = nlp\n",
    "        self.model = model\n",
    "        self.very_similar = 0.5\n",
    "        self.similar = 0.5\n",
    "        \n",
    "\n",
    "    def split_sentences(self, text):\n",
    "        doc = self.nlp(text, disable=[\"ner\"])\n",
    "        roots = [token  for token in doc if token.dep_ == \"ROOT\" ]\n",
    "    \n",
    "        texts = []\n",
    "        for root in roots:\n",
    "            token_list = [e.i for e in root.subtree]\n",
    "            token_list = list(dict.fromkeys(token_list))\n",
    "            token_list.sort()\n",
    "            text = ' '.join([doc[i].text for i in token_list ])\n",
    "            texts.append(text.lower().strip())\n",
    "            \n",
    "        return texts\n",
    "\n",
    "\n",
    "    def insert(self, sentence: str, polarity: int, type: str) -> None:\n",
    "        model = self.model\n",
    "        embeddings = model.encode(sentence)\n",
    "        key = len(self.vectors) + 1\n",
    "        self.vectors[key] = {'text': sentence,\n",
    "                             'polarity': polarity,\n",
    "                             'type': type,\n",
    "                             'vector': embeddings}\n",
    "\n",
    "    def search(self, query: str):\n",
    "        model = self.model\n",
    "        query_vector = model.encode(query)\n",
    "        \n",
    "        similarities = [(key, value['text'],distance.cosine(query_vector, value['vector']),value['polarity'],value['type']) for key, value in self.vectors.items()]\n",
    "        \n",
    "\n",
    "        aux = pd.DataFrame(similarities)\n",
    "        aux.columns = ['index_db','text','similarity','polarity','topic']\n",
    "        aux = aux.sort_values(by=['similarity']).reset_index(drop=True).reset_index()\n",
    "\n",
    "        #aux = aux.reset_index().query('index<20 or similarity<0.7').query('similarity<1')[['index','topic']].groupby(['topic']).count()\n",
    "        \n",
    "        aux = aux.query('index<=10')\n",
    "        #aux = aux.query('similarity <={}'.format(self.very_similar))\n",
    "\n",
    "        aux = aux.query('similarity <={}'.format(self.similar))\n",
    "        \n",
    "        aux = aux[['index','topic']].groupby(['topic']).count()\n",
    "        \n",
    "        \n",
    "        aux['index2'] = aux['index']/aux['index'].sum()\n",
    "     \n",
    "        aux = aux.query('index2>0.6')\n",
    "        \n",
    "\n",
    "        aux = aux.sort_values(by='index', ascending=False).head(1)\n",
    "                \n",
    "        return  list(aux.index.values)\n",
    "\n",
    "    def long_search(self, query: str):\n",
    "        topics = []\n",
    "        for str in self.split_sentences(query):\n",
    "            topics_this = self.search(str)\n",
    "            if len(topics_this)>0:\n",
    "                mini_df = pd.DataFrame(topics_this)\n",
    "                mini_df.columns = ['topic']\n",
    "                mini_df['review'] = query\n",
    "                mini_df['sub_review'] = str\n",
    "                topics.append(mini_df)\n",
    "        if len(topics)>0:\n",
    "            \n",
    "            aux = pd.concat(topics)\n",
    "            #aux ['stars'] = [int(self.sentiment_pipe(str)[0]['label'][0]) for str in aux.sub_review]\n",
    "        else:\n",
    "            aux = None\n",
    "            \n",
    "        return  aux\n",
    "\n",
    "    def set_th(self):\n",
    "        data = pd.DataFrame(self.vectors).transpose()\n",
    "\n",
    "\n",
    "        n = data.shape[0]\n",
    "        if(n>1000):\n",
    "            n = min(max(int(n*0.1),1000),n)\n",
    "            data = data.sample(n)\n",
    "        print('Total size to compute the is {}'.format(data.shape[0]))\n",
    "\n",
    "        same_type_similarity = []\n",
    "        \n",
    "        same_type_top_similarity = []\n",
    "        \n",
    "        for i in range(len(data.vector)):\n",
    "            \n",
    "        \n",
    "            vectors = data.vector\n",
    "            vector = vectors.values[i]\n",
    "            aux = pd.DataFrame(\n",
    "                [distance.cosine(vector, vectors[i]) for i in vectors.keys()]\n",
    "            )\n",
    "            \n",
    "            aux.columns = ['similarity']\n",
    "            \n",
    "            aux['topic'] = data.type.values\n",
    "            \n",
    "            topic_review = data.type.values[i]\n",
    "             \n",
    "            same_type_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,75))\n",
    "        \n",
    "            same_type_top_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,25))\n",
    "        \n",
    "        self.very_similar = np.percentile(same_type_top_similarity,95)\n",
    "        self.similar = np.mean(same_type_similarity)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# use any sentence-transformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\", quantize=True)\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "for db in range(5):\n",
    "\n",
    "    print(f'Cross validation #{db+1} of 5')\n",
    "    sample_size = int(reviews.shape[0]*0.9)\n",
    "    train_reviews = reviews.sample(n=sample_size)\n",
    "    val_reviews = reviews[~reviews.index.isin(train_reviews.index)]\n",
    "    \n",
    "    vector_db = VectorDatabase(nlp, model)\n",
    "    print('uploading vectors to DB')\n",
    "    for index, row in train_reviews.iterrows():\n",
    "        vector_db.insert(row['Review'],None,row['Topic'])\n",
    "    \n",
    "    print('setting thresholds')\n",
    "    vector_db.set_th()\n",
    "    print('seted')\n",
    "    \n",
    "    \n",
    "    guesses = []\n",
    "     \n",
    "    print('Making Classifications')\n",
    "    print(val_reviews.shape[0])\n",
    "    for index, row in val_reviews.iterrows():\n",
    "        review = row['Review']\n",
    "        aux = vector_db.long_search(review)\n",
    "        guess = []\n",
    "        if aux is not None:\n",
    "            guess = aux.topic.values\n",
    "        guesses.append(guess)\n",
    "        \n",
    "    val_reviews['guesses'] = guesses\n",
    "    \n",
    "    \n",
    "    recalls = []\n",
    "    precisions= []\n",
    "    for index, row in val_reviews.iterrows():\n",
    "        recall = row['Topic'] in row['guesses']\n",
    "        precision = np.nan\n",
    "        if len(row['guesses'])>0:\n",
    "            precision = recall\n",
    "            \n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "    \n",
    "    precision = np.nanmean(np.array(precisions))\n",
    "    recall = np.nanmean(np.array(recalls))\n",
    "    \n",
    "    print('recall: {} precision: {}'.format(recall,precision))\n",
    "    results.append((recall,precision))\n",
    "\n",
    "print(pd.DataFrame(results, columns = ['recall','precision']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff311ca2-152a-43d3-906b-aeedc7253de5",
   "metadata": {},
   "source": [
    "# High Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9122c103-1ce5-408c-8e33-af0a87d98994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found at: /Users/mateograciano/.cache/torch/sentence_transformers/sentence-transformers_all-MiniLM-L6-v2/quantized_true.onnx\n",
      "Cross validation #1 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "Making Classifications\n",
      "895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.6916201117318436 precision: 0.9493865030674846\n",
      "Cross validation #2 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "Making Classifications\n",
      "895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.6569832402234637 precision: 0.9333333333333333\n",
      "Cross validation #3 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "Making Classifications\n",
      "895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.6849162011173184 precision: 0.9533437013996889\n",
      "Cross validation #4 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "Making Classifications\n",
      "895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.6770949720670391 precision: 0.9528301886792453\n",
      "Cross validation #5 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "Making Classifications\n",
      "895\n",
      "recall: 0.6636871508379888 precision: 0.9324960753532182\n",
      "CPU times: user 1h 43min 31s, sys: 27.5 s, total: 1h 43min 58s\n",
      "Wall time: 40min 50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.691620</td>\n",
       "      <td>0.949387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.656983</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.684916</td>\n",
       "      <td>0.953344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.677095</td>\n",
       "      <td>0.952830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.663687</td>\n",
       "      <td>0.932496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     recall  precision\n",
       "0  0.691620   0.949387\n",
       "1  0.656983   0.933333\n",
       "2  0.684916   0.953344\n",
       "3  0.677095   0.952830\n",
       "4  0.663687   0.932496"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import spacy\n",
    "from fast_sentence_transformers import FastSentenceTransformer as SentenceTransformer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self,nlp,model):\n",
    "        self.vectors = {}\n",
    "        self.nlp = nlp\n",
    "        self.model = model\n",
    "        self.very_similar = 0.5\n",
    "        self.similar = 0.5\n",
    "        \n",
    "\n",
    "    def split_sentences(self, text):\n",
    "        doc = self.nlp(text, disable=[\"ner\"])\n",
    "        roots = [token  for token in doc if token.dep_ == \"ROOT\" ]\n",
    "    \n",
    "        texts = []\n",
    "        for root in roots:\n",
    "            token_list = [e.i for e in root.subtree]\n",
    "            token_list = list(dict.fromkeys(token_list))\n",
    "            token_list.sort()\n",
    "            text = ' '.join([doc[i].text for i in token_list ])\n",
    "            texts.append(text.lower().strip())\n",
    "            \n",
    "        return texts\n",
    "\n",
    "\n",
    "    def insert(self, sentence: str, polarity: int, type: str) -> None:\n",
    "        model = self.model\n",
    "        embeddings = model.encode(sentence)\n",
    "        key = len(self.vectors) + 1\n",
    "        self.vectors[key] = {'text': sentence,\n",
    "                             'polarity': polarity,\n",
    "                             'type': type,\n",
    "                             'vector': embeddings}\n",
    "\n",
    "    def search(self, query: str):\n",
    "        model = self.model\n",
    "        query_vector = model.encode(query)\n",
    "        \n",
    "        similarities = [(key, value['text'],distance.cosine(query_vector, value['vector']),value['polarity'],value['type']) for key, value in self.vectors.items()]\n",
    "        \n",
    "\n",
    "        aux = pd.DataFrame(similarities)\n",
    "        aux.columns = ['index_db','text','similarity','polarity','topic']\n",
    "        aux = aux.sort_values(by=['similarity']).reset_index(drop=True).reset_index()\n",
    "\n",
    "        #aux = aux.reset_index().query('index<20 or similarity<0.7').query('similarity<1')[['index','topic']].groupby(['topic']).count()\n",
    "        \n",
    "        aux = aux.query('index<=10')\n",
    "        #aux = aux.query('similarity <={}'.format(self.very_similar))\n",
    "\n",
    "        aux = aux.query('similarity <={}'.format(self.similar))\n",
    "        \n",
    "        aux = aux[['index','topic']].groupby(['topic']).count()\n",
    "        \n",
    "        \n",
    "        aux['index2'] = aux['index']/aux['index'].sum()\n",
    "     \n",
    "        aux = aux.query('index2>0.6')\n",
    "        \n",
    "\n",
    "        aux = aux.sort_values(by='index', ascending=False).head(1)\n",
    "                \n",
    "        return  list(aux.index.values)\n",
    "\n",
    "    def long_search(self, query: str):\n",
    "        topics = []\n",
    "        for str in self.split_sentences(query):\n",
    "            topics_this = self.search(str)\n",
    "            if len(topics_this)>0:\n",
    "                mini_df = pd.DataFrame(topics_this)\n",
    "                mini_df.columns = ['topic']\n",
    "                mini_df['review'] = query\n",
    "                mini_df['sub_review'] = str\n",
    "                topics.append(mini_df)\n",
    "        if len(topics)>0:\n",
    "            \n",
    "            aux = pd.concat(topics)\n",
    "            #aux ['stars'] = [int(self.sentiment_pipe(str)[0]['label'][0]) for str in aux.sub_review]\n",
    "        else:\n",
    "            aux = None\n",
    "            \n",
    "        return  aux\n",
    "\n",
    "    def set_th(self):\n",
    "        data = pd.DataFrame(self.vectors).transpose()\n",
    "\n",
    "\n",
    "        n = data.shape[0]\n",
    "        if(n>1000):\n",
    "            n = min(max(int(n*0.1),1000),n)\n",
    "            data = data.sample(n)\n",
    "        print('Total size to compute the is {}'.format(data.shape[0]))\n",
    "\n",
    "        same_type_similarity = []\n",
    "        \n",
    "        same_type_top_similarity = []\n",
    "        \n",
    "        for i in range(len(data.vector)):\n",
    "            \n",
    "        \n",
    "            vectors = data.vector\n",
    "            vector = vectors.values[i]\n",
    "            aux = pd.DataFrame(\n",
    "                [distance.cosine(vector, vectors[i]) for i in vectors.keys()]\n",
    "            )\n",
    "            \n",
    "            aux.columns = ['similarity']\n",
    "            \n",
    "            aux['topic'] = data.type.values\n",
    "            \n",
    "            topic_review = data.type.values[i]\n",
    "             \n",
    "            same_type_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,75))\n",
    "        \n",
    "            same_type_top_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,25))\n",
    "        \n",
    "        self.very_similar = np.percentile(same_type_top_similarity,50)\n",
    "        self.similar = np.mean(same_type_similarity)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# use any sentence-transformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\", quantize=True)\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "for db in range(5):\n",
    "\n",
    "    print(f'Cross validation #{db+1} of 5')\n",
    "    sample_size = int(reviews.shape[0]*0.9)\n",
    "    train_reviews = reviews.sample(n=sample_size)\n",
    "    val_reviews = reviews[~reviews.index.isin(train_reviews.index)]\n",
    "    \n",
    "    vector_db = VectorDatabase(nlp, model)\n",
    "    print('uploading vectors to DB')\n",
    "    for index, row in train_reviews.iterrows():\n",
    "        vector_db.insert(row['Review'],None,row['Topic'])\n",
    "    \n",
    "    print('setting thresholds')\n",
    "    vector_db.set_th()\n",
    "    print('seted')\n",
    "    \n",
    "    \n",
    "    guesses = []\n",
    "     \n",
    "    print('Making Classifications')\n",
    "    print(val_reviews.shape[0])\n",
    "    for index, row in val_reviews.iterrows():\n",
    "        review = row['Review']\n",
    "        aux = vector_db.long_search(review)\n",
    "        guess = []\n",
    "        if aux is not None:\n",
    "            guess = aux.topic.values\n",
    "        guesses.append(guess)\n",
    "        \n",
    "    val_reviews['guesses'] = guesses\n",
    "    \n",
    "    \n",
    "    recalls = []\n",
    "    precisions= []\n",
    "    for index, row in val_reviews.iterrows():\n",
    "        recall = row['Topic'] in row['guesses']\n",
    "        precision = np.nan\n",
    "        if len(row['guesses'])>0:\n",
    "            precision = recall\n",
    "            \n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "    \n",
    "    precision = np.nanmean(np.array(precisions))\n",
    "    recall = np.nanmean(np.array(recalls))\n",
    "    \n",
    "    print('recall: {} precision: {}'.format(recall,precision))\n",
    "    results.append((recall,precision))\n",
    "\n",
    "pd.DataFrame(results, columns = ['recall','precision'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c93e49-df54-42ff-8b91-c9e6e4b2197a",
   "metadata": {},
   "source": [
    "# High recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71e2e850-0c23-4b16-937d-14f5a7226d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found at: /Users/mateograciano/.cache/torch/sentence_transformers/sentence-transformers_all-MiniLM-L6-v2/quantized_true.onnx\n",
      "Cross validation #1 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "Making Classifications\n",
      "895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.8201117318435754 precision: 0.8201117318435754\n",
      "Cross validation #2 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "Making Classifications\n",
      "895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.829050279329609 precision: 0.829050279329609\n",
      "Cross validation #3 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "Making Classifications\n",
      "895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.8189944134078212 precision: 0.8189944134078212\n",
      "Cross validation #4 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "Making Classifications\n",
      "895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.8055865921787709 precision: 0.8055865921787709\n",
      "Cross validation #5 of 5\n",
      "uploading vectors to DB\n",
      "setting thresholds\n",
      "Total size to compute the is 1000\n",
      "seted\n",
      "Making Classifications\n",
      "895\n",
      "recall: 0.8122905027932961 precision: 0.8122905027932961\n",
      "CPU times: user 2h 5min 19s, sys: 34.5 s, total: 2h 5min 54s\n",
      "Wall time: 46min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.820112</td>\n",
       "      <td>0.820112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.829050</td>\n",
       "      <td>0.829050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.818994</td>\n",
       "      <td>0.818994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.805587</td>\n",
       "      <td>0.805587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.812291</td>\n",
       "      <td>0.812291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     recall  precision\n",
       "0  0.820112   0.820112\n",
       "1  0.829050   0.829050\n",
       "2  0.818994   0.818994\n",
       "3  0.805587   0.805587\n",
       "4  0.812291   0.812291"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import spacy\n",
    "from fast_sentence_transformers import FastSentenceTransformer as SentenceTransformer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self,nlp,model):\n",
    "        self.vectors = {}\n",
    "        self.nlp = nlp\n",
    "        self.model = model\n",
    "        self.very_similar = 0.5\n",
    "        self.similar = 0.5\n",
    "        \n",
    "\n",
    "    def split_sentences(self, text):\n",
    "        doc = self.nlp(text, disable=[\"ner\"])\n",
    "        roots = [token  for token in doc if token.dep_ == \"ROOT\" ]\n",
    "    \n",
    "        texts = []\n",
    "        for root in roots:\n",
    "            token_list = [e.i for e in root.subtree]\n",
    "            token_list = list(dict.fromkeys(token_list))\n",
    "            token_list.sort()\n",
    "            text = ' '.join([doc[i].text for i in token_list ])\n",
    "            texts.append(text.lower().strip())\n",
    "            \n",
    "        return texts\n",
    "\n",
    "\n",
    "    def insert(self, sentence: str, polarity: int, type: str) -> None:\n",
    "        model = self.model\n",
    "        embeddings = model.encode(sentence)\n",
    "        key = len(self.vectors) + 1\n",
    "        self.vectors[key] = {'text': sentence,\n",
    "                             'polarity': polarity,\n",
    "                             'type': type,\n",
    "                             'vector': embeddings}\n",
    "\n",
    "    def search(self, query: str):\n",
    "        model = self.model\n",
    "        query_vector = model.encode(query)\n",
    "        \n",
    "        similarities = [(key, value['text'],distance.cosine(query_vector, value['vector']),value['polarity'],value['type']) for key, value in self.vectors.items()]\n",
    "        \n",
    "\n",
    "        aux = pd.DataFrame(similarities)\n",
    "        aux.columns = ['index_db','text','similarity','polarity','topic']\n",
    "        aux = aux.sort_values(by=['similarity']).reset_index(drop=True).reset_index()\n",
    "\n",
    "        #aux = aux.reset_index().query('index<20 or similarity<0.7').query('similarity<1')[['index','topic']].groupby(['topic']).count()\n",
    "        \n",
    "        aux = aux.query('index<=10')\n",
    "        #aux = aux.query('similarity <={}'.format(self.very_similar))\n",
    "\n",
    "        aux = aux.query('similarity <={}'.format(self.similar))\n",
    "        \n",
    "        aux = aux[['index','topic']].groupby(['topic']).count()\n",
    "        \n",
    "        \n",
    "        aux['index2'] = aux['index']/aux['index'].sum()\n",
    "     \n",
    "        #aux = aux.query('index2>0.6')\n",
    "        \n",
    "\n",
    "        aux = aux.sort_values(by='index', ascending=False).head(1)\n",
    "                \n",
    "        return  list(aux.index.values)\n",
    "\n",
    "    def long_search(self, query: str):\n",
    "        topics = []\n",
    "        for str in self.split_sentences(query):\n",
    "            topics_this = self.search(str)\n",
    "            if len(topics_this)>0:\n",
    "                mini_df = pd.DataFrame(topics_this)\n",
    "                mini_df.columns = ['topic']\n",
    "                mini_df['review'] = query\n",
    "                mini_df['sub_review'] = str\n",
    "                topics.append(mini_df)\n",
    "        if len(topics)>0:\n",
    "            \n",
    "            aux = pd.concat(topics)\n",
    "            #aux ['stars'] = [int(self.sentiment_pipe(str)[0]['label'][0]) for str in aux.sub_review]\n",
    "        else:\n",
    "            aux = None\n",
    "            \n",
    "        return  aux\n",
    "\n",
    "    def set_th(self):\n",
    "        data = pd.DataFrame(self.vectors).transpose()\n",
    "\n",
    "\n",
    "        n = data.shape[0]\n",
    "        if(n>1000):\n",
    "            n = min(max(int(n*0.1),1000),n)\n",
    "            data = data.sample(n)\n",
    "        print('Total size to compute the is {}'.format(data.shape[0]))\n",
    "\n",
    "        same_type_similarity = []\n",
    "        \n",
    "        same_type_top_similarity = []\n",
    "        \n",
    "        for i in range(len(data.vector)):\n",
    "            \n",
    "        \n",
    "            vectors = data.vector\n",
    "            vector = vectors.values[i]\n",
    "            aux = pd.DataFrame(\n",
    "                [distance.cosine(vector, vectors[i]) for i in vectors.keys()]\n",
    "            )\n",
    "            \n",
    "            aux.columns = ['similarity']\n",
    "            \n",
    "            aux['topic'] = data.type.values\n",
    "            \n",
    "            topic_review = data.type.values[i]\n",
    "             \n",
    "            same_type_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,75))\n",
    "        \n",
    "            same_type_top_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,25))\n",
    "        \n",
    "        self.very_similar = np.percentile(same_type_top_similarity,75)\n",
    "        self.similar = np.mean(same_type_similarity)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# use any sentence-transformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\", quantize=True)\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "for db in range(5):\n",
    "\n",
    "    print(f'Cross validation #{db+1} of 5')\n",
    "    sample_size = int(reviews.shape[0]*0.9)\n",
    "    train_reviews = reviews.sample(n=sample_size)\n",
    "    val_reviews = reviews[~reviews.index.isin(train_reviews.index)]\n",
    "    \n",
    "    vector_db = VectorDatabase(nlp, model)\n",
    "    print('uploading vectors to DB')\n",
    "    for index, row in train_reviews.iterrows():\n",
    "        vector_db.insert(row['Review'],None,row['Topic'])\n",
    "    \n",
    "    print('setting thresholds')\n",
    "    vector_db.set_th()\n",
    "    print('seted')\n",
    "    \n",
    "    \n",
    "    guesses = []\n",
    "     \n",
    "    print('Making Classifications')\n",
    "    print(val_reviews.shape[0])\n",
    "    for index, row in val_reviews.iterrows():\n",
    "        review = row['Review']\n",
    "        aux = vector_db.long_search(review)\n",
    "        guess = []\n",
    "        if aux is not None:\n",
    "            guess = aux.topic.values\n",
    "        guesses.append(guess)\n",
    "        \n",
    "    val_reviews['guesses'] = guesses\n",
    "    \n",
    "    \n",
    "    recalls = []\n",
    "    precisions= []\n",
    "    for index, row in val_reviews.iterrows():\n",
    "        recall = row['Topic'] in row['guesses']\n",
    "        precision = np.nan\n",
    "        if len(row['guesses'])>0:\n",
    "            precision = recall\n",
    "            \n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "    \n",
    "    precision = np.nanmean(np.array(precisions))\n",
    "    recall = np.nanmean(np.array(recalls))\n",
    "    \n",
    "    print('recall: {} precision: {}'.format(recall,precision))\n",
    "    results.append((recall,precision))\n",
    "\n",
    "pd.DataFrame(results, columns = ['recall','precision'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
