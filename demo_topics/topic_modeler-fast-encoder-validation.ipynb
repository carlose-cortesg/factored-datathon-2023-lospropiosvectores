{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a122381b-7163-4a63-96b3-df59286ba955",
   "metadata": {},
   "source": [
    "# Syntetic reviews of fashions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbea87b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "from fast_sentence_transformers import FastSentenceTransformer as SentenceTransformer\n",
    "from transformers import pipeline\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from upload_data.utils import execute_query\n",
    "\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self,nlp,model):\n",
    "        self.vectors = {}\n",
    "        self.nlp = nlp\n",
    "        self.model = model\n",
    "        self.very_similar = 0.5\n",
    "        self.similar = 0.8\n",
    "        \n",
    "\n",
    "    def split_sentences(self, text):\n",
    "        doc = self.nlp(text, disable=[\"ner\"])\n",
    "        roots = [token  for token in doc if token.dep_ == \"ROOT\" ]\n",
    "    \n",
    "        texts = []\n",
    "        for root in roots:\n",
    "            token_list = [e.i for e in root.subtree]\n",
    "            token_list = list(dict.fromkeys(token_list))\n",
    "            token_list.sort()\n",
    "            text = ' '.join([doc[i].text for i in token_list ])\n",
    "            texts.append(text.lower().strip())\n",
    "            \n",
    "        return texts\n",
    "\n",
    "\n",
    "    def insert(self, sentence: str, type: str) -> None:\n",
    "        model = self.model\n",
    "        embeddings = model.encode(sentence, normalize_embeddings = True)\n",
    "        key = len(self.vectors) + 1\n",
    "        self.vectors[key] = {'text': sentence,\n",
    "                             'type': type,\n",
    "                             'vector': embeddings}\n",
    "\n",
    "    def search(self, query: str):\n",
    "        model = self.model\n",
    "        query_vector = model.encode(query)\n",
    "        \n",
    "        similarities = [(key, value['text'],distance.cosine(query_vector, value['vector']), value['type']) for key, value in self.vectors.items()]\n",
    "        \n",
    "        aux = pd.DataFrame(similarities)\n",
    "        aux.columns = ['index_db','text','similarity','topic']\n",
    "        aux = aux.sort_values(by=['similarity']).reset_index(drop=True).reset_index()\n",
    "        \n",
    "        sims = 1/(1+aux['similarity'].values)\n",
    "        norm_sim = sims/np.sum(sims)\n",
    "        information = -np.log2(norm_sim)\n",
    "        entropy = np.sum(norm_sim*information)\n",
    "        \n",
    "        aux = aux.query('index<=10')\n",
    "        aux = aux.query('similarity <={}'.format(self.similar))\n",
    "        aux = aux[['index','topic']].groupby(['topic']).count()\n",
    "\n",
    "        aux = aux.sort_values(by='index', ascending=False).head(1)\n",
    "                \n",
    "        return  list(aux.index.values), entropy\n",
    "\n",
    "    def long_search(self, query: str):\n",
    "        topics = []\n",
    "        avg_entropy = []\n",
    "        for str in self.split_sentences(query):\n",
    "            topics_this, entropy = self.search(str)\n",
    "            if len(topics_this)>0:\n",
    "                mini_df = pd.DataFrame(topics_this)\n",
    "                mini_df.columns = ['topic']\n",
    "                mini_df['review'] = query\n",
    "                mini_df['sub_review'] = str\n",
    "                topics.append(mini_df)\n",
    "                avg_entropy.append(entropy)\n",
    "        if len(topics)>0:\n",
    "            \n",
    "            aux = pd.concat(topics)\n",
    "        else:\n",
    "            aux = None\n",
    "        \n",
    "        avg_entropy = np.mean(avg_entropy)\n",
    "        return  aux, avg_entropy\n",
    "\n",
    "    def set_th(self):\n",
    "        data = pd.DataFrame(self.vectors).transpose()\n",
    "\n",
    "        same_type_similarity = []\n",
    "        \n",
    "        same_type_top_similarity = []\n",
    "        \n",
    "        for i in range(len(data.vector)):\n",
    "        \n",
    "            vectors = data.vector\n",
    "            vector = vectors.values[i]\n",
    "            aux = pd.DataFrame(\n",
    "                [distance.cosine(vector, vectors[i]) for i in vectors.keys()]\n",
    "            )\n",
    "            \n",
    "            aux.columns = ['similarity']\n",
    "            \n",
    "            aux['topic'] = data.type.values\n",
    "            \n",
    "            topic_review = data.type.values[i]\n",
    "             \n",
    "            same_type_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,75))\n",
    "        \n",
    "            same_type_top_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,25))\n",
    "        \n",
    "        self.very_similar = np.percentile(same_type_top_similarity,95)\n",
    "        self.similar = np.mean(same_type_similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7338f64f",
   "metadata": {},
   "source": [
    "#### Load models: \n",
    "\n",
    "Syntethic reviews, model and nlp preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2c1c26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found at: C:\\Users\\david/.cache\\torch\\sentence_transformers\\sentence-transformers_all-MiniLM-L6-v2\\quantized_true.onnx\n"
     ]
    }
   ],
   "source": [
    "with open('syn_reviews.json', 'r') as file:\n",
    "    syn_reviews = json.load(file)\n",
    "    file.close()\n",
    "    \n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\", quantize=True)\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dff27b",
   "metadata": {},
   "source": [
    "#### Train vector base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3dfda520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading vectors to DB\n"
     ]
    }
   ],
   "source": [
    "vector_db = VectorDatabase(nlp, model)\n",
    "print('uploading vectors to DB')\n",
    "for topic, reviews in syn_reviews.items():\n",
    "    for review in reviews:\n",
    "        vector_db.insert(review,topic)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d5e09a1",
   "metadata": {},
   "source": [
    "vectors = {}\n",
    "for index, topic in enumerate(syn_reviews.keys()):\n",
    "    \n",
    "    centroid = {'text': f'Centroid of {topic}',\n",
    "                'type': topic,\n",
    "                'vector': np.mean([element['vector'] \n",
    "                                   for element in vector_db.vectors.values() \n",
    "                                   if element['type'] == topic], axis = 0)}\n",
    "    vectors[index + 1] = centroid\n",
    "\n",
    "vector_db.vectors = vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fad2103",
   "metadata": {},
   "source": [
    "#### Extract real data and predict on these reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e9e0b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT DISTINCT reviewText\n",
    "FROM `plenary-stacker-393921.factored.raw_reviews` TABLESAMPLE SYSTEM (5 PERCENT)\n",
    "WHERE asin = 'B00M4NF9H0' AND reviewText IS NOT NULL\n",
    "LIMIT 220\n",
    "'''\n",
    "\n",
    "val_reviews = execute_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f52d4858",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "guesses = []\n",
    "entropy = []\n",
    "for _, row in val_reviews.iterrows():\n",
    "    review = row['reviewText']\n",
    "    aux, avg_entropy = vector_db.long_search(review)\n",
    "    guess = []\n",
    "    if aux is not None:\n",
    "        guess = aux.topic.values\n",
    "    guesses.append(guess)\n",
    "    entropy.append(avg_entropy)\n",
    "    \n",
    "val_reviews['guesses'] = guesses\n",
    "val_reviews['entropy'] = entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfc9202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_reviews.to_excel('prueba.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6496721",
   "metadata": {},
   "source": [
    "#### Zero shot learning base model: Bart model tuned by Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a7598f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "candidate_labels = ['Design','Brand','Packaging and Presentation','Price and Value','Customer Support','User Experience','Material and Quality','Fit and Comfort','Versatility','Longevity']\n",
    "\n",
    "result_val = val_reviews['reviewText'].apply(lambda x: classifier(x, candidate_labels, multiclass = True))\n",
    "val_reviews['model'] = [[label for score, label in zip(result['scores'], result['labels']) if score > 0.15] for result in result_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f558c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_reviews.to_excel('two_models.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134a020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_reviews = pd.read_excel('two_models.xlsx')\n",
    "val_reviews['guesses'] = val_reviews['guesses'].apply(lambda x: x.replace(\"['\",\"\").replace(\"']\",\"\").replace(\"\\n\", \"\").split(\"' '\"))\n",
    "val_reviews['model'] = val_reviews['model'].apply(lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ef64555",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_reviews.to_excel('comparison.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
