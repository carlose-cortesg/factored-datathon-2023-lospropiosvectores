{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a122381b-7163-4a63-96b3-df59286ba955",
   "metadata": {},
   "source": [
    "# Syntetic reviews of fashions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19029324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "from fast_sentence_transformers import FastSentenceTransformer as SentenceTransformer\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from upload_data.utils import execute_query\n",
    "\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self,nlp,model):\n",
    "        self.vectors = {}\n",
    "        self.nlp = nlp\n",
    "        self.model = model\n",
    "        self.very_similar = 0.5\n",
    "        self.similar = 0.5\n",
    "        \n",
    "\n",
    "    def split_sentences(self, text):\n",
    "        doc = self.nlp(text, disable=[\"ner\"])\n",
    "        roots = [token  for token in doc if token.dep_ == \"ROOT\" ]\n",
    "    \n",
    "        texts = []\n",
    "        for root in roots:\n",
    "            token_list = [e.i for e in root.subtree]\n",
    "            token_list = list(dict.fromkeys(token_list))\n",
    "            token_list.sort()\n",
    "            text = ' '.join([doc[i].text for i in token_list ])\n",
    "            texts.append(text.lower().strip())\n",
    "            \n",
    "        return texts\n",
    "\n",
    "\n",
    "    def insert(self, sentence: str, polarity: int, type: str) -> None:\n",
    "        model = self.model\n",
    "        embeddings = model.encode(sentence)\n",
    "        key = len(self.vectors) + 1\n",
    "        self.vectors[key] = {'text': sentence,\n",
    "                             'polarity': polarity,\n",
    "                             'type': type,\n",
    "                             'vector': embeddings}\n",
    "\n",
    "    def search(self, query: str):\n",
    "        model = self.model\n",
    "        query_vector = model.encode(query)\n",
    "        \n",
    "        similarities = [(key, value['text'],distance.cosine(query_vector, value['vector']),value['polarity'],value['type']) for key, value in self.vectors.items()]\n",
    "        \n",
    "\n",
    "        aux = pd.DataFrame(similarities)\n",
    "        aux.columns = ['index_db','text','similarity','polarity','topic']\n",
    "        aux = aux.sort_values(by=['similarity']).reset_index(drop=True).reset_index()\n",
    "\n",
    "        #aux = aux.reset_index().query('index<20 or similarity<0.7').query('similarity<1')[['index','topic']].groupby(['topic']).count()\n",
    "        \n",
    "        aux = aux.query('index<=10')\n",
    "        #aux = aux.query('similarity <={}'.format(self.very_similar))\n",
    "\n",
    "        aux = aux.query('similarity <={}'.format(self.similar))\n",
    "        \n",
    "        aux = aux[['index','topic']].groupby(['topic']).count()\n",
    "        \n",
    "        \n",
    "        #aux['index2'] = aux['index']/aux['index'].sum()\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        aux = aux.sort_values(by='index', ascending=False).head(1)\n",
    "                \n",
    "        return  list(aux.index.values)\n",
    "\n",
    "    def long_search(self, query: str):\n",
    "        topics = []\n",
    "        for str in self.split_sentences(query):\n",
    "            topics_this = self.search(str)\n",
    "            if len(topics_this)>0:\n",
    "                mini_df = pd.DataFrame(topics_this)\n",
    "                mini_df.columns = ['topic']\n",
    "                mini_df['review'] = query\n",
    "                mini_df['sub_review'] = str\n",
    "                topics.append(mini_df)\n",
    "        if len(topics)>0:\n",
    "            \n",
    "            aux = pd.concat(topics)\n",
    "            #aux ['stars'] = [int(self.sentiment_pipe(str)[0]['label'][0]) for str in aux.sub_review]\n",
    "        else:\n",
    "            aux = None\n",
    "            \n",
    "        return  aux\n",
    "\n",
    "    def set_th(self):\n",
    "        data = pd.DataFrame(self.vectors).transpose()\n",
    "\n",
    "        same_type_similarity = []\n",
    "        \n",
    "        same_type_top_similarity = []\n",
    "        \n",
    "        for i in range(len(data.vector)):\n",
    "        \n",
    "            vectors = data.vector\n",
    "            vector = vectors.values[i]\n",
    "            aux = pd.DataFrame(\n",
    "                [distance.cosine(vector, vectors[i]) for i in vectors.keys()]\n",
    "            )\n",
    "            \n",
    "            aux.columns = ['similarity']\n",
    "            \n",
    "            aux['topic'] = data.type.values\n",
    "            \n",
    "            topic_review = data.type.values[i]\n",
    "             \n",
    "            same_type_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,75))\n",
    "        \n",
    "            same_type_top_similarity.append(np.percentile(aux.query(f'topic==\"{topic_review}\"').similarity,25))\n",
    "        \n",
    "        self.very_similar = np.percentile(same_type_top_similarity,95)\n",
    "        self.similar = np.mean(same_type_similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f39370f",
   "metadata": {},
   "source": [
    "#### Load models: \n",
    "\n",
    "Syntethic reviews, model and nlp preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2c1c26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found at: C:\\Users\\david/.cache\\torch\\sentence_transformers\\sentence-transformers_all-MiniLM-L6-v2\\quantized_true.onnx\n"
     ]
    }
   ],
   "source": [
    "reviews = pd.read_csv('../Syntetic_reviews/reviews_all.csv')\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\", quantize=True)\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dff27b",
   "metadata": {},
   "source": [
    "#### Train vector base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbbb8e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading vectors to DB\n"
     ]
    }
   ],
   "source": [
    "vector_db = VectorDatabase(nlp, model)\n",
    "print('uploading vectors to DB')\n",
    "for _, row in reviews.iterrows():\n",
    "    vector_db.insert(row['Review'],row['Polarity'],row['Topic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fad2103",
   "metadata": {},
   "source": [
    "#### Extract real data and predict on these reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e9e0b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT DISTINCT reviewText\n",
    "FROM `plenary-stacker-393921.factored.raw_reviews` TABLESAMPLE SYSTEM (5 PERCENT)\n",
    "WHERE asin = 'B00M4NF9H0' AND reviewText IS NOT NULL\n",
    "LIMIT 220\n",
    "'''\n",
    "\n",
    "val_reviews = execute_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9529f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses = []\n",
    "\n",
    "for _, row in val_reviews.iterrows():\n",
    "    review = row['reviewText']\n",
    "    aux = vector_db.long_search(review)\n",
    "    guess = []\n",
    "    if aux is not None:\n",
    "        guess = aux.topic.values\n",
    "    guesses.append(guess)\n",
    "    \n",
    "val_reviews['guesses'] = guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "50a6c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_reviews.loc[val_reviews['guesses'].apply(lambda x: str(x)) == '[]', 'reviewText'].to_excel('prueba.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb70f201",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5231\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.map_locations\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas._libs.index.IndexEngine._call_map_locations'\n",
      "Traceback (most recent call last):\n",
      "  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 5231, in pandas._libs.hashtable.PyObjectHashTable.map_locations\n",
      "TypeError: unhashable type: 'list'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]                                                          147\n",
       "[Fit and Comfort]                                            46\n",
       "[Longevity]                                                   4\n",
       "[Material and Quality]                                        3\n",
       "[Price and Value]                                             2\n",
       "[Fit and Comfort, Fit and Comfort]                            1\n",
       "[Fit and Comfort, Fit and Comfort, Fit and Comfort]           1\n",
       "[Fit and Comfort, Fit and Comfort]                            1\n",
       "[Fit and Comfort, Fit and Comfort]                            1\n",
       "[Fit and Comfort, Fit and Comfort]                            1\n",
       "[Price and Value, Fit and Comfort]                            1\n",
       "[Fit and Comfort, Fit and Comfort]                            1\n",
       "[Fit and Comfort, Price and Value]                            1\n",
       "[Fit and Comfort, Fit and Comfort]                            1\n",
       "[Fit and Comfort, Fit and Comfort]                            1\n",
       "[Fit and Comfort, Fit and Comfort]                            1\n",
       "[Versatility, Fit and Comfort]                                1\n",
       "[Fit and Comfort, Fit and Comfort]                            1\n",
       "[Fit and Comfort, Fit and Comfort]                            1\n",
       "[Fit and Comfort, Fit and Comfort]                            1\n",
       "[Fit and Comfort, Material and Quality, Fit and Comfort]      1\n",
       "[Fit and Comfort, User Experience, Fit and Comfort]           1\n",
       "[Packaging and Presentation]                                  1\n",
       "Name: guesses, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_reviews.guesses.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
